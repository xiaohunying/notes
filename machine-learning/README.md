# Supervised Learning

## Regression Problem

### Linear Regression

Hypothesis Function: &nbsp; $h_{\theta}(x)=\theta_{0}+\theta_{1}x$

Parameters: &nbsp; $\theta_{0}$, $\theta_{1}$

Cost Function: &nbsp; $J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{i})^{2}$

Goal: &nbsp; $\underset{\theta}{minimize}\J(\theta)$

### Polynomial Regression

## Classification Problem

### Logistic Regression

#### One-vs-All (One-vs-Rest)

multi-class

### SVM: Support Vector Machine Hypothesis

### Neural Network

- activation units
- bias nodes x_{0}

# Unsupervised Learning

## Clustering

### K-Means

# PCA: Principal Component Analysis

Reduce data from n-dimensions to k-dimensions

## Application: Dimensionality Reduction

- Data Compression
- Data Visualization

## [u, s, v] = svd(sigma)

\[U_{reduce} = u(:, 1:k)\]

Z = U_{reduce}' * x; (~~x_{0}=1~~)

```math
Z = U_{reduce}
$$a^2$$
```

~~~equation
Z = U_{reduce}
$$a^2$$
~~~

$$a^2$$